{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537ebdf1",
   "metadata": {},
   "source": [
    "# DSC 540 Week 10\n",
    "Kannur, Gyan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce3efc",
   "metadata": {},
   "source": [
    "**Activity 9: Extracting the Top 100 eBooks from Gutenberg**\n",
    "\n",
    "Head over to the supplied Jupyter notebook (in the GitHub repository) to work on this activity. These are the steps that will help you solve this activity:\n",
    "\n",
    "• Import the necessary libraries, including regex and beautifulsoup.\n",
    "\n",
    "• Check the SSL certificate.\n",
    "\n",
    "• Read the HTML from the URL. Write a small function to check the status of the web request.\n",
    "\n",
    "• Decode the response and pass this on to BeautifulSoup for HTML parsing.\n",
    "\n",
    "• Find all the href tags and store them in the list of links.\n",
    "\n",
    "• Check what the list looks like – print the first 30 elements.\n",
    "\n",
    "• Use a regular expression to find the numeric digits in these links. These are the file numbers for the top 100 eBooks.\n",
    "\n",
    "• Initialize the empty list to hold the file numbers over an appropriate range and use regex to find the numeric digits in the link href string.\n",
    "\n",
    "• Use the findall method. What does the soup object’s text look like? Use the .text method and print only the first 2,000 characters (do not print the whole thing, as it is too long).\n",
    "\n",
    "• Search in the extracted text (using a regular expression) from the soup object to find the names of the top 100 eBooks (yesterday’s ranking).\n",
    "\n",
    "• Create a starting index. It should point at the text Top 100 Ebooks yesterday.\n",
    "\n",
    "• Use the splitlines method of soup.text. It splits the lines of text of the soup object. Loop 1-100 to add the strings of the next 100 lines to this temporary list. Hint: use the splitlines method. Use a regular expression to extract only text from the name strings and append it to an empty list.\n",
    "\n",
    "• Use match and span to find the indices and use them."
   ]
  },
  {
   "cell_type": "code",
   "id": "19ffd4cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.087918Z",
     "start_time": "2024-12-15T22:44:30.703136Z"
    }
   },
   "source": [
    "# Load the necessary libraries.\n",
    "import urllib.request, urllib.parse, urllib.error \n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import ssl \n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "9c792a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.113768Z",
     "start_time": "2024-12-15T22:44:31.089444Z"
    }
   },
   "source": [
    "#There may be certificate error due to invalid certificates, Ignore this error \n",
    "certx = ssl.create_default_context() \n",
    "certx.check_hostname = False \n",
    "certx.verify_mode = ssl.CERT_NONE"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ce13f8ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.117929Z",
     "start_time": "2024-12-15T22:44:31.113768Z"
    }
   },
   "source": [
    "# function to check the response status code, 200 means OK, any other status means the request failed \n",
    "\n",
    "def status_check(r):\n",
    "    if r.status_code==200:\n",
    "        print(\"Success!\") \n",
    "        return 1 \n",
    "    else:\n",
    "        print(\"Failed!\") \n",
    "        return -1"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "eff26297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.367332Z",
     "start_time": "2024-12-15T22:44:31.117929Z"
    }
   },
   "source": [
    "# Read the HTML from the URL and check the response \n",
    "gutenburgurl = 'https://www.gutenberg.org/browse/scores/top' \n",
    "response = requests.get(gutenburgurl) \n",
    "status_check(response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "156e57b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.407921Z",
     "start_time": "2024-12-15T22:44:31.370369Z"
    }
   },
   "source": [
    "# Decode the response and pass on to BeautifulSoup for HTML parsing \n",
    "\n",
    "urlContent = response.content.decode(response.encoding) \n",
    "soup = BeautifulSoup(urlContent, 'html.parser')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "31b28b68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.415320Z",
     "start_time": "2024-12-15T22:44:31.407921Z"
    }
   },
   "source": [
    "# Find all the href tags and store them in the list of links. \n",
    "# Empty list to hold all the http links in the HTML page \n",
    "\n",
    "href_list=[] \n",
    "\n",
    "for item in soup.find_all('a'):\n",
    "    href_list.append(item.get('href'))\n",
    "    \n",
    "# Check what the list looks like – print the first 30 elements. \n",
    "href_list[:30]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/about/',\n",
       " '/about/',\n",
       " '/policy/collection_development.html',\n",
       " '/about/contact_information.html',\n",
       " '/about/background/',\n",
       " '/policy/permission.html',\n",
       " '/policy/privacy_policy.html',\n",
       " '/policy/terms_of_use.html',\n",
       " '/ebooks/',\n",
       " '/ebooks/',\n",
       " '/ebooks/bookshelf/',\n",
       " '/browse/scores/top',\n",
       " '/ebooks/offline_catalogs.html',\n",
       " '/help/',\n",
       " '/help/',\n",
       " '/help/copyright.html',\n",
       " '/help/errata.html',\n",
       " '/help/file_formats.html',\n",
       " '/help/faq.html',\n",
       " '/policy/',\n",
       " '/help/public_domain_ebook_submission.html',\n",
       " '/help/submitting_your_own_work.html',\n",
       " '/help/mobile.html',\n",
       " '/attic/',\n",
       " '/donate/',\n",
       " '/donate/',\n",
       " 'pretty-pictures',\n",
       " '#books-last1',\n",
       " '#authors-last1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "3572ae6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.421434Z",
     "start_time": "2024-12-15T22:44:31.416338Z"
    }
   },
   "source": [
    "#Use a regular expression to find the numeric digits in these links. \n",
    "# These are the file numbers for the top 100 eBooks.\n",
    "\n",
    "#Initialize the empty list to hold the file numbers over an appropriate range and use regex to \n",
    "# find the numeric digits in the link href string \n",
    "# Number 19 to 119 in the original list of links have the Top 100 ebooksnumber.\n",
    "\n",
    "filenum =[] \n",
    "\n",
    "for i in range(19,119):\n",
    "    link=href_list[i]\n",
    "    link=link.strip()\n",
    "    # Use the findall method. What does the soup object's text look like?\n",
    "\n",
    "    num=re.findall('[0-9]+',link) \n",
    "    if len(num)==1:\n",
    "        # Append the filenumber casted as integer \n",
    "        filenum.append(int(num[0]))\n",
    "        \n",
    "# Print the file numbers \n",
    "print(\"file numbers for the top 100 ebooks on Gutenberg are\\n\") \n",
    "print(filenum)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file numbers for the top 100 ebooks on Gutenberg are\n",
      "\n",
      "[1, 1, 7, 7, 30, 30, 25558, 27104, 84, 2701, 1513, 46, 145, 100, 2641, 37106, 67979, 16389, 394, 6761, 6593, 1259, 2160, 4085, 5197, 11, 1342, 174, 2554, 25344, 64317, 2542, 5200, 98, 76, 844, 43, 28054, 1400, 2650, 345, 1998, 1232, 3201, 74890, 50150, 2000, 1184, 2600, 3207, 1080, 1661, 55, 4363, 5740, 16119, 4300, 2591, 1260, 1952, 1497, 74897, 74892, 74893, 205, 30254, 27827, 6130, 408, 45, 36034, 135, 3296, 768, 8492, 219, 74896, 33283, 24022, 244, 2680, 74, 7370, 514, 31284, 74891, 1727, 996, 34901, 2781, 74899]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "02f5aca9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.429449Z",
     "start_time": "2024-12-15T22:44:31.422985Z"
    }
   },
   "source": [
    "# Use the .text method and print only the first 2,000 characters \n",
    "# (do not print the whole thing, as it is too long). \n",
    "\n",
    "print(soup.text[:2000])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 100 | Project Gutenberg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Menu▾\n",
      "\n",
      "\n",
      "\n",
      "About\n",
      "          ▾\n",
      "\n",
      "▾\n",
      "\n",
      "\n",
      "About Project Gutenberg\n",
      "Collection Development\n",
      "Contact Us\n",
      "History & Philosophy\n",
      "Permissions & License\n",
      "Privacy Policy\n",
      "Terms of Use\n",
      "\n",
      "\n",
      "\n",
      "Search and Browse\n",
      "      \t  ▾\n",
      "\n",
      "▾\n",
      "\n",
      "\n",
      "Book Search\n",
      "Bookshelves\n",
      "Frequently Downloaded\n",
      "Offline Catalogs\n",
      "\n",
      "\n",
      "\n",
      "Help\n",
      "          ▾\n",
      "\n",
      "▾\n",
      "\n",
      "\n",
      "All help topics →\n",
      "Copyright How-To\n",
      "Errata, Fixes and Bug Reports\n",
      "File Formats\n",
      "Frequently Asked Questions\n",
      "Policies →\n",
      "Public Domain eBook Submission\n",
      "Submitting Your Own Work\n",
      "Tablets, Phones and eReaders\n",
      "The Attic →\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ways to donate\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "To determine the ranking we count the times each file gets downloaded.\n",
      "Both HTTP and FTP transfers are counted.\n",
      "Only transfers from ibiblio.org are counted as we have no access to our mirrors log files.\n",
      "Multiple downloads from the same IP address on the same day count as one download.\n",
      "IP addresses that download more than 100 files a day are considered\n",
      "robots and are not considered.\n",
      "Books made out of multiple files like most audio books are counted\n",
      "if any file is downloaded.\n",
      "\n",
      "Downloaded Books\n",
      "2024-12-14404564\n",
      "last 7 days3319543\n",
      "last 30 days13680338\n",
      "\n",
      "Pretty Pictures\n",
      "\n",
      "Top 100 EBooks yesterday —\n",
      "  Top 100 Authors yesterday —\n",
      "  Top 100 EBooks last 7 days —\n",
      "  Top 100 Authors last 7 days —\n",
      "  Top 100 EBooks last 30 days —\n",
      "  Top 100 Authors last 30 days\n",
      "\n",
      "Top 100 EBooks yesterday\n",
      "\n",
      "呻吟語 by Kun Lü (20918)\n",
      "歸蓮夢 by active 18th century Su'anzhuren (4999)\n",
      "Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (2359)\n",
      "Moby Dick; Or, The Whale by Herman Melville (2220)\n",
      "Romeo and Juliet by William Shakespeare (2082)\n",
      "A Christmas Carol in Prose; Being a Ghost Story of Christmas by Charles Dickens (1975)\n",
      "Middlemarch by George Eliot (1651)\n",
      "The Complete Works of William Shakespeare by William Shakespeare (1627)\n",
      "A Room with a View by E. M.  Forster (1626)\n",
      "Little Women; Or, Meg, Jo, Beth, and Amy by Louisa May Alcott (1549)\n",
      "The Blue Castle: a novel by L. M.  Montgomery (1437)\n",
      "The Enchanted A\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "d6c7173c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.489364Z",
     "start_time": "2024-12-15T22:44:31.430534Z"
    }
   },
   "source": [
    "# Search in the extracted text (using a regular expression) \n",
    "# from the soup object to find the names of the top 100 eBooks (yesterday's ranking).\n",
    "\n",
    "# Initialize a temp list to hold titles \n",
    "title_list_temp =[]\n",
    "\n",
    "# Creating a start index pointing at the text \"Top 100 Ebooks yesterday\"\n",
    "\n",
    "index_start = soup.text.splitlines().index('Top 100 EBooks yesterday')\n",
    "\n",
    "# Loop 1-100 to add the strings of next 100 lines to this temporary list. \n",
    "\n",
    "for item in range(107):\n",
    "    title_list_temp.append(soup.text.splitlines()[index_start+2+item])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "5d520bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.496663Z",
     "start_time": "2024-12-15T22:44:31.489364Z"
    }
   },
   "source": [
    "# Use regular expression to extract only text from the name strings and append to an empty list\n",
    "\n",
    "title_list=[] \n",
    "for i in range(7,107):\n",
    "    id1,id2=re.match('^[a-zA-Z ]*',title_list_temp[i]).span() \n",
    "    title_list.append(title_list_temp[i][id1:id2])\n",
    "    \n",
    "# Printing List of Titles \n",
    "for item in title_list:\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Complete Works of William Shakespeare by William Shakespeare \n",
      "A Room with a View by E\n",
      "Little Women\n",
      "The Blue Castle\n",
      "The Enchanted April by Elizabeth Von Arnim \n",
      "Cranford by Elizabeth Cleghorn Gaskell \n",
      "The Adventures of Ferdinand Count Fathom \n",
      "History of Tom Jones\n",
      "Twenty years after by Alexandre Dumas and Auguste Maquet \n",
      "The Expedition of Humphry Clinker by T\n",
      "The Adventures of Roderick Random by T\n",
      "My Life \n",
      "Alice\n",
      "Pride and Prejudice by Jane Austen \n",
      "The Picture of Dorian Gray by Oscar Wilde \n",
      "Crime and Punishment by Fyodor Dostoyevsky \n",
      "The Scarlet Letter by Nathaniel Hawthorne \n",
      "The Great Gatsby by F\n",
      "A Doll\n",
      "Metamorphosis by Franz Kafka \n",
      "A Tale of Two Cities by Charles Dickens \n",
      "Adventures of Huckleberry Finn by Mark Twain \n",
      "The Importance of Being Earnest\n",
      "The Strange Case of Dr\n",
      "The Brothers Karamazov by Fyodor Dostoyevsky \n",
      "Great Expectations by Charles Dickens \n",
      "Du c\n",
      "Dracula by Bram Stoker \n",
      "Thus Spake Zarathustra\n",
      "The Prince by Niccol\n",
      "Moby Word Lists by Grady Ward \n",
      "Forty\n",
      "The Devil is an Ass by Ben Jonson \n",
      "Don Quijote by Miguel de Cervantes Saavedra \n",
      "The Count of Monte Cristo by Alexandre Dumas and Auguste Maquet \n",
      "War and Peace by graf Leo Tolstoy \n",
      "Leviathan by Thomas Hobbes \n",
      "A Modest Proposal by Jonathan Swift \n",
      "The Adventures of Sherlock Holmes by Arthur Conan Doyle \n",
      "The Wonderful Wizard of Oz by L\n",
      "Beyond Good and Evil by Friedrich Wilhelm Nietzsche \n",
      "Tractatus Logico\n",
      "Doctrina Christiana \n",
      "Ulysses by James Joyce \n",
      "Grimms\n",
      "Jane Eyre\n",
      "The Yellow Wallpaper by Charlotte Perkins Gilman \n",
      "The Republic by Plato \n",
      "Chinese Fables and Folk Stories by Chow\n",
      "Twelve poems by Edith Wharton \n",
      "Cousin\n",
      "Walden\n",
      "The Romance of Lust\n",
      "The Kama Sutra of Vatsyayana by Vatsyayana \n",
      "The Iliad by Homer \n",
      "The Souls of Black Folk by W\n",
      "Anne of Green Gables by L\n",
      "White Nights and Other Stories by Fyodor Dostoyevsky \n",
      "Les Mis\n",
      "The Confessions of St\n",
      "Wuthering Heights by Emily Bront\n",
      "The King in Yellow by Robert W\n",
      "Heart of Darkness by Joseph Conrad \n",
      "A gentle pioneer \n",
      "Calculus Made Easy by Silvanus P\n",
      "A Christmas Carol by Charles Dickens \n",
      "A Study in Scarlet by Arthur Conan Doyle \n",
      "Meditations by Emperor of Rome Marcus Aurelius \n",
      "The Adventures of Tom Sawyer\n",
      "Second Treatise of Government by John Locke \n",
      "Little Women by Louisa May Alcott \n",
      "Josefine Mutzenbacher by Felix Salten \n",
      "Historical record of the Ninety\n",
      "The Odyssey by Homer \n",
      "Don Quixote by Miguel de Cervantes Saavedra \n",
      "On Liberty by John Stuart Mill \n",
      "Just so stories by Rudyard Kipling \n",
      "The behaviour book \n",
      "The Philippines a Century Hence by Jos\n",
      "Anna Karenina by graf Leo Tolstoy \n",
      "Les mis\n",
      "Novo dicion\n",
      "The divine comedy by Dante Alighieri \n",
      "Winnie\n",
      "The Legend of Sleepy Hollow by Washington Irving \n",
      "The Problems of Philosophy by Bertrand Russell \n",
      "A Christmas Carol by Charles Dickens \n",
      "The Tragical History of Doctor Faustus by Christopher Marlowe \n",
      "Frankenstein\n",
      "Aristoteles\n",
      "Notes from the Underground by Fyodor Dostoyevsky \n",
      "\n",
      "The Critique of Pure Reason by Immanuel Kant \n",
      "\n",
      "\n",
      "Top \n",
      "  Top \n",
      "  Top \n",
      "  Top \n",
      "  Top \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "134e0427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.500585Z",
     "start_time": "2024-12-15T22:44:31.497719Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "5f797933",
   "metadata": {},
   "source": [
    "**Activity 10: Building Your Own Movie Database by Reading an API**\n",
    "\n",
    "The aims of this activity are as follows: To retrieve and print basic data about a movie (the title is entered by the user) from the web (OMDb database) If a poster of the movie can be found, it downloads the file and saves it at a user-specified location These are the steps that will help you solve this activity:\n",
    "\n",
    "• Import urllib.request, urllib.parse, urllib.error, and json.\n",
    "• Load the secret API key (you have to get one from the OMDb website and use that; it has a daily limit of 1,000) from a JSON file stored in the same folder in a variable, by using json.loads.\n",
    "\n",
    "• Obtain a key and store it in JSON as APIkeys.json.\n",
    "\n",
    "• Open the APIkeys.json file.\n",
    "\n",
    "• Assign the OMDb portal (http://www.omdbapi.com/?) as a string to a variable.\n",
    "\n",
    "• Create a variable called apikey with the last portion of the URL (&apikey=secretapikey), where secretapikey is your own API key.\n",
    "\n",
    "• Write a utility function called print_json to print the movie data from a JSON file (which we will get from the portal).\n",
    "\n",
    "• Write a utility function to download a poster of the movie based on the information from the JSON dataset and save it in your local folder. Use the os module. The poster data is stored in the JSON key Poster. Use the Python command to open a file and write the poster data. Close the file after you’re done. This function will save the poster data as an image file.\n",
    "\n",
    "• Write a utility function called search_movie to search for a movie by its name, print the downloaded JSON data, and save the movie poster in the local folder. Use a try-except loop for this. Use the previously created serviceurl and apikey variables. You have to pass on a dictionary with a key, t, and the movie name as the corresponding value to the urllib.parse.urlencode() function and then add the serviceurl and apikey to the output of the function to construct the full URL. This URL will be used to access the data. The JSON data has a key called Response. If it is True, that means the read was successful. Check this before processing the data. If it’s not successful, then print the JSON key Error, which will contain the appropriate error message returned by the movie database.\n",
    "\n",
    "• Test the search_movie function by entering Titanic.\n",
    "\n",
    "• Test the search_movie function by entering “Random_error” (obviously, this will not be found, and you should be able to check whether your error catching code is working properly)."
   ]
  },
  {
   "cell_type": "code",
   "id": "3e390273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.506105Z",
     "start_time": "2024-12-15T22:44:31.502101Z"
    }
   },
   "source": [
    "# Loading Libraries \n",
    "import urllib.request, urllib.parse, urllib.error \n",
    "import json \n",
    "from PIL import Image \n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "64513092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.513720Z",
     "start_time": "2024-12-15T22:44:31.506869Z"
    }
   },
   "source": [
    "# read the api key file to get the key \n",
    "# Opening JSON file \n",
    "f = open('./datasets/APIKeys.json')\n",
    "\n",
    "# returns JSON object as \n",
    "# a dictionary \n",
    "# getting the API Key from file \n",
    "apiKey = \"\" \n",
    "apiKeyDict = json.load(f)\n",
    "\n",
    "for item in apiKeyDict.items():\n",
    "    apiKey = str(item[1])"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "bc86d118",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.521104Z",
     "start_time": "2024-12-15T22:44:31.514745Z"
    }
   },
   "source": [
    "# Assing the portal value to variable \n",
    "omdbBaseURL = \"http://www.omdbapi.com/?\"\n",
    "\n",
    "# variable api key to hold the key value \n",
    "apikey = \"&apikey=\" + apiKey"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "a9985817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.527906Z",
     "start_time": "2024-12-15T22:44:31.522635Z"
    }
   },
   "source": [
    "# Write a utility function called print_json to print the movie data from a JSON file \n",
    "# (which we will get from the portal).\n",
    "\n",
    "def print_json(data):\n",
    "    for item in data.items():\n",
    "        keyData = str(item[0]) \n",
    "        valData = str(item[1]) \n",
    "        if keyData == \"Poster\":\n",
    "            # poster contains image, we would download an show the image \n",
    "            im = Image.open(requests.get(valData, stream=True).raw) \n",
    "            im.show() \n",
    "        else:\n",
    "            print(keyData + \" : \" + valData + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "4aff28af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:31.531907Z",
     "start_time": "2024-12-15T22:44:31.527906Z"
    }
   },
   "source": [
    "# method to get movie details\n",
    "\n",
    "def search_movie(moviename, serviceurl, apikey):\n",
    "    try:\n",
    "        apiURl = serviceurl + \"t=\" + moviename + apikey\n",
    "        # calling the api to get response \n",
    "        with urllib.request.urlopen(apiURl) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            \n",
    "        # check the status of the response \n",
    "        status = False \n",
    "        for item in data.items():\n",
    "            if (str(item[0]).lower() == 'response' and str(item[1]).lower() =='true'): \n",
    "                status = True\n",
    "                \n",
    "        # checking the status of response \n",
    "        if(status == False):\n",
    "            print('Api returned failure response') \n",
    "        else:\n",
    "            # if response is successful then print the data\n",
    "            print_json(data)\n",
    "            \n",
    "    except:\n",
    "        # print an error message if connection not sucessful \n",
    "        print(\"Error occured while processing your request\")"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "e7480ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:32.007613Z",
     "start_time": "2024-12-15T22:44:31.531907Z"
    }
   },
   "source": [
    "# Test the search_movie function by entering Titanic. \n",
    "search_movie('Titanic', omdbBaseURL, apikey)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Titanic\n",
      "\n",
      "Year : 1997\n",
      "\n",
      "Rated : PG-13\n",
      "\n",
      "Released : 19 Dec 1997\n",
      "\n",
      "Runtime : 194 min\n",
      "\n",
      "Genre : Drama, Romance\n",
      "\n",
      "Director : James Cameron\n",
      "\n",
      "Writer : James Cameron\n",
      "\n",
      "Actors : Leonardo DiCaprio, Kate Winslet, Billy Zane\n",
      "\n",
      "Plot : A seventeen-year-old aristocrat falls in love with a kind but poor artist aboard the luxurious, ill-fated R.M.S. Titanic.\n",
      "\n",
      "Language : English, Swedish, Italian, French\n",
      "\n",
      "Country : United States, Mexico\n",
      "\n",
      "Awards : Won 11 Oscars. 126 wins & 83 nominations total\n",
      "\n",
      "Ratings : [{'Source': 'Internet Movie Database', 'Value': '7.9/10'}, {'Source': 'Rotten Tomatoes', 'Value': '88%'}, {'Source': 'Metacritic', 'Value': '75/100'}]\n",
      "\n",
      "Metascore : 75\n",
      "\n",
      "imdbRating : 7.9\n",
      "\n",
      "imdbVotes : 1,313,284\n",
      "\n",
      "imdbID : tt0120338\n",
      "\n",
      "Type : movie\n",
      "\n",
      "DVD : N/A\n",
      "\n",
      "BoxOffice : $674,292,608\n",
      "\n",
      "Production : N/A\n",
      "\n",
      "Website : N/A\n",
      "\n",
      "Response : True\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "7e751401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:32.197468Z",
     "start_time": "2024-12-15T22:44:32.010147Z"
    }
   },
   "source": [
    "# Test the search_movie function by entering \"Random_error\" (obviously, this will not be found, and you should be able to check \n",
    "# whether your error catching code is working properly). \n",
    "\n",
    "search_movie('Random_error', omdbBaseURL, apikey)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api returned failure response\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "0c40701e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:32.202412Z",
     "start_time": "2024-12-15T22:44:32.197468Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "e17b0580",
   "metadata": {},
   "source": [
    "**Connect to the Twitter API and do a simple data pull**\n",
    "\n",
    "• If you don’t have a twitter account – create one at twitter.com/signup (you can delete the account after this assignment)\n",
    "\n",
    "• Sign in to apps.twitter.com\n",
    "\n",
    "• Click “Create New App”\n",
    "\n",
    "• Give your app a name and description\n",
    "\n",
    "• Agree to the developer agreement – you will want to make sure to indicate this is for a class project, and this step can take several days to get through, so don’t wait until last minute to complete this portion of the assignment\n",
    "\n",
    "• Create an access token\n",
    "\n",
    "• You should receive a consumer key and a token\n",
    "\n",
    "• Using either the instructions from the book on connecting to an API or for help look here\n",
    "\n",
    "– pull back data searching for “Bellevue University” and “Data Science” (or something else you are interested in)\n",
    "\n",
    "– How to Create a Twitter App and API Interface via Python. (Grogan, 2016)\n",
    "\n",
    "– Welcome Python-Twitter’s Documentation! (The Python-Twitter Developers, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "id": "59b39e26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:32.340679Z",
     "start_time": "2024-12-15T22:44:32.202412Z"
    }
   },
   "source": [
    "# Load Libraries\n",
    "\n",
    "# Import the Twython class \n",
    "from twython import Twython \n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "2e9000d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T22:44:32.717456Z",
     "start_time": "2024-12-15T22:44:32.340679Z"
    }
   },
   "source": [
    "# Load credentials from json file\n",
    "# import the Twython class, instantiate an object of it, and create our search query.\n",
    "\n",
    "# We'll use only four arguments in the query: q, result_type, count and lang respectively \n",
    "# for the search keyword, type, count, and language of results \n",
    "\n",
    "with open(\"TwitterAPIKeys.json\", \"r\") as file:\n",
    "    creds = json.load(file) \n",
    "\n",
    "# Instantiate an object \n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET']) \n",
    "\n",
    "# Create our query \n",
    "query = {'q': 'learn python',\n",
    "        'result_type': 'popular',\n",
    "        'count': 10,\n",
    "        'lang': 'en',\n",
    "}"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'TwitterAPIKeys.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 7\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load credentials from json file\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# import the Twython class, instantiate an object of it, and create our search query.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# We'll use only four arguments in the query: q, result_type, count and lang respectively \u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# for the search keyword, type, count, and language of results \u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTwitterAPIKeys.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m      8\u001B[0m     creds \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file) \n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Instantiate an object \u001B[39;00m\n",
      "File \u001B[1;32m~\\gyan-python-workspace\\jup-workspace\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'TwitterAPIKeys.json'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "3462bc5c",
   "metadata": {},
   "source": [
    "# we can use our Twython object to call the search method \n",
    "# which returns a dictionary of search_metadata and statuses - the queried results.\n",
    "\n",
    "# We'll only look at the statuses part, and save a portion of all information in a pandas dataframe, \n",
    "# to present it in a table.\n",
    "\n",
    "# Search tweets \n",
    "dict_ = {'user': [], 'date': [], 'text': [], 'favorite_count': []} \n",
    "\n",
    "for status in python_tweets.search(**query)['statuses']:\n",
    "    dict_['user'].append(status['user']['screen_name']) \n",
    "    dict_['date'].append(status['created_at']) \n",
    "    dict_['text'].append(status['text']) \n",
    "    dict_['favorite_count'].append(status['favorite_count'])\n",
    "\n",
    "# Structure data in a pandas DataFrame for easier manipulation \n",
    "df = pd.DataFrame(dict_) \n",
    "df.sort_values(by='favorite_count', inplace=True, ascending=False) \n",
    "df.head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ddbc6a0",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c9ce658",
   "metadata": {},
   "source": [
    "**Using one of the datasets provided in Weeks 7 & 8, or a dataset of your own, choose 3 of the following visualizations to complete. You must submit via PDF along with your code. You are free to use Matplotlib, Seaborn or another package if you prefer.**\n",
    "\n",
    "• Line\n",
    "\n",
    "• Scatter\n",
    "\n",
    "• Bar\n",
    "\n",
    "• Histogram\n",
    "\n",
    "• Density Plot\n",
    "\n",
    "• Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "id": "5218367b",
   "metadata": {},
   "source": [
    "# I am using Iris dataset to demonstrate Python data visualizations \n",
    "# First, we'll import pandas, a data processing and CSV file I/O library \n",
    "import pandas as pd\n",
    "\n",
    "# We'll also import seaborn, a Python graphing library \n",
    "\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore \n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "\n",
    "# Next, we'll load the Iris flower dataset, which is in the \"../input/\" directory \n",
    "iris = pd.read_csv(\"Iris.csv\") # the iris dataset is now a Pandas DataFrame\n",
    "\n",
    "# Let's see what's in the iris data - Jupyter notebooks print the result of the last thing you do \n",
    "iris.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "042f1cb9",
   "metadata": {},
   "source": [
    "# Let's see how many examples we have of each species \n",
    "iris[\"variety\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4055e50",
   "metadata": {},
   "source": [
    "# The first way we can plot things is using the .plot extension from Pandas dataframes \n",
    "# We'll use this to make a scatterplot of the Iris features. \n",
    "iris.plot(kind=\"scatter\", x=\"sepal.length\", y=\"sepal.width\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c33e1b5",
   "metadata": {},
   "source": [
    "iris.plot(kind=\"line\", x=\"sepal.length\", y=\"sepal.width\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2fe9e10",
   "metadata": {},
   "source": [
    "# We can also use the seaborn library to make a similar plot \n",
    "# A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure \n",
    "\n",
    "sns.jointplot(x=\"sepal.length\", y=\"sepal.width\", data=iris, size=5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2342192f",
   "metadata": {},
   "source": [
    "# One piece of information missing in the plots above is what species each plant is \n",
    "# We'll use seaborn's FacetGrid to color the scatterplot by species \n",
    "\n",
    "sns.FacetGrid(iris, hue=\"variety\", size=5) \\\n",
    ".map(plt.scatter, \"sepal.length\", \"sepal.width\") \\\n",
    ".add_legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19e52737",
   "metadata": {},
   "source": [
    "# We can look at an individual feature in Seaborn through a boxplot \n",
    "sns.boxplot(x=\"variety\", y=\"petal.length\", data=iris)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a71ae591",
   "metadata": {},
   "source": [
    "# One way we can extend this plot is adding a layer of individual points on top\n",
    "# it through Seaborn's striplot \n",
    "# # We'll use jitter=True so that all the points don't fall in single vertical lines \n",
    "# above the species \n",
    "# # Saving the resulting axes as ax each time causes the resulting plot to be shown \n",
    "# on top of the previous axes\n",
    "\n",
    "ax = sns.boxplot(x=\"variety\", y=\"petal.length\", data=iris)\n",
    "\n",
    "ax = sns.stripplot(x=\"variety\", y=\"petal.length\", data=iris, jitter=True,edgecolor=\"gray\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a7d24699",
   "metadata": {},
   "source": [
    "# A violin plot combines the benefits of the previous two plots and simplifies them \n",
    "# Denser regions of the data are fatter, and sparser thiner in a violin plot.\n",
    "\n",
    "sns.violinplot(x=\"variety\", y=\"petal.length\", data=iris, size=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6aa28b20",
   "metadata": {},
   "source": [
    "# A final seaborn plot useful for looking at univariate relations is the kdeplot, \n",
    "# which creates and visualizes a kernel density estimate of the underlying feature \n",
    "\n",
    "sns.FacetGrid(iris, hue=\"variety\", size=6).map(sns.kdeplot, \"petal.length\").add_legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21bd589f",
   "metadata": {},
   "source": [
    "iris.reset_index().head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9689c893",
   "metadata": {},
   "source": [
    "# Another useful seaborn plot is the pairplot, which shows the bivariate relation \n",
    "# between each pair of features \n",
    "# # From the pairplot, we'll see that the Iris-setosa species is separataed from the other \n",
    "# two across all feature combinations \n",
    "\n",
    "sns.pairplot(iris, hue=\"variety\", size=3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "819e96cf",
   "metadata": {},
   "source": [
    "# The diagonal elements in a pairplot show the histogram by default \n",
    "# We can update these elements to show other things, such as a kde \n",
    "\n",
    "sns.pairplot(iris, hue=\"variety\", size=3, diag_kind=\"kde\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "194aac4e",
   "metadata": {},
   "source": [
    "# Now that we've covered seaborn, let's go back to some of the ones we can make with Pandas \n",
    "# We can quickly make a boxplot with Pandas on each feature split out by species \n",
    "\n",
    "iris.boxplot(by=\"variety\", figsize=(12, 6))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af765182",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
